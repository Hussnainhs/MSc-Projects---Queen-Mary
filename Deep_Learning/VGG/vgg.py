# -*- coding: utf-8 -*-
"""VGG_cw3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e49Lbn2y0VYKcBEoRqL6-VSYBmN3JFXu
"""

# Commented out IPython magic to ensure Python compatibility.
# Imports
from keras.layers.normalization import BatchNormalization
import tensorflow as tf
import matplotlib.pyplot as plt
import keras,os
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Activation
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.datasets import mnist
import numpy as np

# %matplotlib inline
np.random.seed(1)

# Loading dataset and split into training and testing set

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

#Add channels dimension if it is missing
if (X_train.ndim == 3):
  X_train = np.expand_dims(X_train, -1)
if (X_test.ndim == 3):
  X_test = np.expand_dims(X_test, -1)

print ("number of training examples = " + str(X_train.shape[0]))
print ("number of evaluation examples = " + str(Y_train.shape[0]))
print ("X_train shape: " + str(X_train.shape))
print ("Y_train shape: " + str(Y_train.shape))
print ("X_test shape: " + str(X_test.shape))
print ("Y_test shape: " + str(Y_test.shape))

#Build Model
model = Sequential()

#Initialize input shape parameters
input_height, input_width, input_channels = 28, 28, 1

#block 1
model.add(Conv2D(input_shape=(input_height,input_width, input_channels),filters=64,kernel_size=(3,3),padding="same"))
model.add(Activation("relu"))
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
#block 2
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
#block 3
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
#block 4
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
#Flatten Layer and Fully Connected Layers
model.add(Flatten())
model.add(Dense(units=512,activation="relu"))
model.add(Dense(units=512,activation="relu"))
model.add(Dense(units=10, activation="softmax"))

model.summary()

from keras.optimizers import SGD
#Choose optimizer and compile model
opt = SGD(lr=0.0001)
model.compile(optimizer=opt, loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])

from keras.preprocessing.image import ImageDataGenerator
aug = ImageDataGenerator()
#Set Training Parameters
EPOCHS = 50
BS = 32 #Batch Size
N = X_train.shape[0] # Number of training samples
N_val = X_test.shape[0] # Number of validation samples
#Train Model
hist = model.fit_generator(aug.flow(X_train, Y_train, batch_size=BS), validation_data=(X_test, Y_test), steps_per_epoch= N / BS, validation_steps= N_val / BS, epochs=EPOCHS)

#Plot training and testing Accuracy
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('CIFAR10 - VGG Accuracy - Learning Rate = 0.0001')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='best')
plt.grid()
plt.show()

#Plot training and testing Loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('CIFAR10 - VGG Loss - Learning Rate = 0.0001')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='best')
plt.grid()
plt.show()

#Prediction on real data using trained MNIST model 
import cv2
img = cv2.imread('manual_data/9.png')
#Convert to grayscale
grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#Add missing dimensions (batch size and numner of channels) 
x = np.expand_dims(grayscale, axis=0)
x = np.expand_dims(x, axis=3)
#Get Prediction
digit = model.predict_classes(x)
print(digit)